{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Momentum:\n",
    "\n",
    "    \"\"\"모멘텀 SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "\n",
    "        \n",
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {}\n",
    "optimizers['Momentum'] = Momentum()\n",
    "optimizers['AdaGrad'] = AdaGrad()\n",
    "optimizers['Adam'] = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size_list, output_size,\n",
    "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.params = {}\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.__init_weight(weight_init_std)\n",
    "\n",
    "        # 계층 생성\n",
    "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
    "        self.layers = OrderedDict()\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "                                                      self.params['b' + str(idx)])\n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "            self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def __init_weight(self, weight_init_std):\n",
    "        \n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLU를 사용할 때의 권장 초깃값\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoid를 사용할 때의 권장 초깃값\n",
    "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \n",
    "        y = self.predict(x)\n",
    "\n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
    "\n",
    "        return self.last_layer.forward(y, t) + weight_decay\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))   \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n",
    "class Dropout:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key in optimizers.keys():\n",
    "    networks[key] = MultiLayerNet(\n",
    "        input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "        output_size=10)\n",
    "    train_loss[key] = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
    "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
    "hidden_layer_size = 5  # 은닉층이 5개\n",
    "activations = {}  # 이곳에 활성화 결과를 저장\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
    "    w = np.random.randn(node_num, node_num) * 1\n",
    "    # w = np.random.randn(node_num, node_num) * 0.01\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "\n",
    "    # 활성화 함수도 바꿔가며 실험해보자！\n",
    "    z = sigmoid(a)\n",
    "    # z = ReLU(a)\n",
    "    # z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "\n",
    "# 히스토그램 그리기\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    # plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 배치 정규화\n",
    "\n",
    "학습 속도 개선(learning rate를 크게 할 수 있음), 가중치 초기값 선택의 의존성이 낮아짐\n",
    "과적합 위험을 줄인다. 기울기 소실 문제 해결\n",
    "\n",
    "Affine -> batch norm -> relu 가 층마다 반복\n",
    "(활성화 앞인지 뒤인지는 실험이 진행되고 있음)\n",
    "\n",
    "초기값을 잘 지정하지 않는 이상 배치 정규화를 하지 않으면 학습이 잘 진행되지 않음\n",
    "평균이 0 분산이 1이 되도록 정규화(Standard)\n",
    "\n",
    "이렇게만 하면 활성화 함수의 비선형성을 없앨 수도 있음\n",
    "-> scale과 shift개념을 도입해 오처역전파 과정에서 학습\n",
    "\n",
    "test 에서는 앞서 학습에서 저장해둔 mean과 variance를 사용해 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
